{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721af6cb",
   "metadata": {},
   "source": [
    "|평가문항|상세기준|\n",
    "|:--|:--|\n",
    "|1. tfrecord를 활용한 데이터셋 구성과 전처리를 통해 프로젝트 베이스라인 구성을 확인하였다.|MPII 데이터셋을 기반으로 1epoch에 30분 이내에 학습가능한 베이스라인을 구축하였다.|\n",
    "|2. simplebaseline 모델을 정상적으로 구현하였다.|simplebaseline 모델을 구현하여 실습코드의 모델을 대체하여 정상적으로 학습이 진행되었다.|\n",
    "|3. Hourglass 모델과 simplebaseline 모델을 비교분석한 결과를 체계적으로 정리하였다.|두 모델의 pose estimation 테스트결과 이미지 및 학습진행상황 등을 체계적으로 비교분석하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f775b",
   "metadata": {},
   "source": [
    "## Simple baseline\n",
    "![](https://velog.velcdn.com/images/xpelqpdj0422/post/7989c3b8-0f98-41e1-866c-5aa263eef7ca/image.png)\n",
    "\n",
    "\n",
    "### Resnet-50\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9LqUp7XyEx1QNc6A.png)\n",
    "\n",
    "``ResNet(C5)``에 ``deconvolutional layer``가 더해졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742a5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a203791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5b7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json 파일로부터 필요한 정보를 파싱하는 함수\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afdcc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a91a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7d716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8916264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ebffa01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nray.init()\\n\\nprint('Start to parse annotations.')\\nif not os.path.exists(TFRECORD_PATH):\\n    os.makedirs(TFRECORD_PATH)\\n\\nwith open(TRAIN_JSON) as train_json:\\n    train_annos = json.load(train_json)\\n    train_annotations = [\\n        parse_one_annotation(anno, IMAGE_PATH)\\n        for anno in train_annos\\n    ]\\n    print('First train annotation: ', train_annotations[0])\\n\\nwith open(VALID_JSON) as val_json:\\n    val_annos = json.load(val_json)\\n    val_annotations = [\\n        parse_one_annotation(anno, IMAGE_PATH) \\n        for anno in val_annos\\n    ]\\n    print('First val annotation: ', val_annotations[0])\\n    \\nprint('Start to build TF Records.')\\nbuild_tf_records(train_annotations, num_train_shards, 'train')\\nbuild_tf_records(val_annotations, num_val_shards, 'val')\\n\\nprint('Successfully wrote {} annotations to TF Records.'.format(\\n    len(train_annotations) + len(val_annotations)))\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "'''\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d939d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033004e9",
   "metadata": {},
   "source": [
    "### Hourglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e61413df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef259924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19efc0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8af88261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dacbd7e",
   "metadata": {},
   "source": [
    "### Simple baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c8269f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09c769ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simplebaseline(input_shape=(256, 256, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')(inputs)\n",
    "    x = _make_deconv_layer(3)(x)\n",
    "    out = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, out, name='simple_baseline')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c9c44d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights=\\'imagenet\\')\\n\\ndef _make_deconv_layer(num_deconv_layers):\\n    seq_model = tf.keras.models.Sequential()\\n    for i in range(num_deconv_layers):\\n        # upsampling을 위해 \"Conv2DTranspose\"를 수행합니다. Standard Conv2D는 \"Downsampling\" 수행\\n        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding=\\'same\\'))\\n        seq_model.add(tf.keras.layers.BatchNormalization())\\n        seq_model.add(tf.keras.layers.ReLU())\\n    return seq_model\\n\\nupconv = _make_deconv_layer(3)\\n\\nfinal_layer = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding=\\'same\\')\\n\\n\\ndef Simplebaseline(input_shape=(256, 256, 3)):\\n    inputs = tf.keras.Input(shape=input_shape)\\n    x = resnet(inputs)\\n    x = upconv(x)\\n    out = final_layer(x)\\n    model = tf.keras.Model(inputs, out, name=\\'simple_baseline\\')\\n    return model\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        # upsampling을 위해 \"Conv2DTranspose\"를 수행합니다. Standard Conv2D는 \"Downsampling\" 수행\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model\n",
    "\n",
    "upconv = _make_deconv_layer(3)\n",
    "\n",
    "final_layer = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')\n",
    "\n",
    "\n",
    "def Simplebaseline(input_shape=(256, 256, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = resnet(inputs)\n",
    "    x = upconv(x)\n",
    "    out = final_layer(x)\n",
    "    model = tf.keras.Model(inputs, out, name='simple_baseline')\n",
    "    return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9c5bffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_baseline\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, None, None, 2048)  23587712  \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 64, 64, 256)       10489600  \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 16)        4112      \n",
      "=================================================================\n",
      "Total params: 34,081,424\n",
      "Trainable params: 34,026,768\n",
      "Non-trainable params: 54,656\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model = Simplebaseline(input_shape=(256, 256, 3))\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6acf88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, output):\n",
    "        weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "        loss = tf.math.reduce_mean(\n",
    "            tf.math.square(labels - output) * weights) * (\n",
    "                1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "            \n",
    "            start_time = time.time()\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {} and time {}'.format(epoch, train_loss, (time.time()-start_time)))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e451f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ee831d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = Simplebaseline(IMAGE_SHAPE)\n",
    "        # model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b2824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 0.684519589 epoch total loss 0.684519589\n",
      "Trained batch 2 batch loss 0.665708899 epoch total loss 0.675114274\n",
      "Trained batch 3 batch loss 0.560857594 epoch total loss 0.637028694\n",
      "Trained batch 4 batch loss 0.519033551 epoch total loss 0.607529879\n",
      "Trained batch 5 batch loss 0.503195882 epoch total loss 0.586663067\n",
      "Trained batch 6 batch loss 0.476881981 epoch total loss 0.56836623\n",
      "Trained batch 7 batch loss 0.405674368 epoch total loss 0.545124531\n",
      "Trained batch 8 batch loss 0.437824398 epoch total loss 0.531712\n",
      "Trained batch 9 batch loss 0.401551664 epoch total loss 0.517249763\n",
      "Trained batch 10 batch loss 0.378966957 epoch total loss 0.503421426\n",
      "Trained batch 11 batch loss 0.38383615 epoch total loss 0.492550075\n",
      "Trained batch 12 batch loss 0.370633334 epoch total loss 0.482390314\n",
      "Trained batch 13 batch loss 0.350639 epoch total loss 0.472255588\n",
      "Trained batch 14 batch loss 0.337896049 epoch total loss 0.462658465\n",
      "Trained batch 15 batch loss 0.358273506 epoch total loss 0.455699474\n",
      "Trained batch 16 batch loss 0.374345958 epoch total loss 0.45061487\n",
      "Trained batch 17 batch loss 0.404806376 epoch total loss 0.447920233\n",
      "Trained batch 18 batch loss 0.434772313 epoch total loss 0.447189808\n",
      "Trained batch 19 batch loss 0.430413514 epoch total loss 0.446306825\n",
      "Trained batch 20 batch loss 0.433272779 epoch total loss 0.445655107\n",
      "Trained batch 21 batch loss 0.431106389 epoch total loss 0.444962323\n",
      "Trained batch 22 batch loss 0.428783387 epoch total loss 0.444226921\n",
      "Trained batch 23 batch loss 0.390385598 epoch total loss 0.441885978\n",
      "Trained batch 24 batch loss 0.414063036 epoch total loss 0.440726727\n",
      "Trained batch 25 batch loss 0.406723559 epoch total loss 0.439366609\n",
      "Trained batch 26 batch loss 0.401783377 epoch total loss 0.437921077\n",
      "Trained batch 27 batch loss 0.423531353 epoch total loss 0.437388152\n",
      "Trained batch 28 batch loss 0.420162588 epoch total loss 0.436772913\n",
      "Trained batch 29 batch loss 0.415288657 epoch total loss 0.436032087\n",
      "Trained batch 30 batch loss 0.406048268 epoch total loss 0.435032636\n",
      "Trained batch 31 batch loss 0.407480538 epoch total loss 0.434143841\n",
      "Trained batch 32 batch loss 0.399326503 epoch total loss 0.433055788\n",
      "Trained batch 33 batch loss 0.419636 epoch total loss 0.432649136\n",
      "Trained batch 34 batch loss 0.394270778 epoch total loss 0.431520343\n",
      "Trained batch 35 batch loss 0.403341979 epoch total loss 0.430715263\n",
      "Trained batch 36 batch loss 0.404304177 epoch total loss 0.429981619\n",
      "Trained batch 37 batch loss 0.417214215 epoch total loss 0.429636568\n",
      "Trained batch 38 batch loss 0.417233229 epoch total loss 0.429310143\n",
      "Trained batch 39 batch loss 0.452729523 epoch total loss 0.42991066\n",
      "Trained batch 40 batch loss 0.4495233 epoch total loss 0.430401\n",
      "Trained batch 41 batch loss 0.428528637 epoch total loss 0.43035531\n",
      "Trained batch 42 batch loss 0.416227937 epoch total loss 0.430018932\n",
      "Trained batch 43 batch loss 0.397802532 epoch total loss 0.429269701\n",
      "Trained batch 44 batch loss 0.393236607 epoch total loss 0.428450763\n",
      "Trained batch 45 batch loss 0.371494681 epoch total loss 0.427185059\n",
      "Trained batch 46 batch loss 0.38245374 epoch total loss 0.426212639\n",
      "Trained batch 47 batch loss 0.374981642 epoch total loss 0.425122619\n",
      "Trained batch 48 batch loss 0.362030417 epoch total loss 0.423808187\n",
      "Trained batch 49 batch loss 0.362815797 epoch total loss 0.422563434\n",
      "Trained batch 50 batch loss 0.339788198 epoch total loss 0.420907944\n",
      "Trained batch 51 batch loss 0.394804716 epoch total loss 0.42039609\n",
      "Trained batch 52 batch loss 0.425523728 epoch total loss 0.420494705\n",
      "Trained batch 53 batch loss 0.417933226 epoch total loss 0.420446366\n",
      "Trained batch 54 batch loss 0.404897749 epoch total loss 0.420158416\n",
      "Trained batch 55 batch loss 0.401763529 epoch total loss 0.419823974\n",
      "Trained batch 56 batch loss 0.397498488 epoch total loss 0.419425309\n",
      "Trained batch 57 batch loss 0.397090852 epoch total loss 0.419033498\n",
      "Trained batch 58 batch loss 0.368543744 epoch total loss 0.418162972\n",
      "Trained batch 59 batch loss 0.365194619 epoch total loss 0.417265207\n",
      "Trained batch 60 batch loss 0.381713033 epoch total loss 0.416672677\n",
      "Trained batch 61 batch loss 0.396737039 epoch total loss 0.416345835\n",
      "Trained batch 62 batch loss 0.396746814 epoch total loss 0.416029751\n",
      "Trained batch 63 batch loss 0.389349699 epoch total loss 0.41560623\n",
      "Trained batch 64 batch loss 0.397443563 epoch total loss 0.415322453\n",
      "Trained batch 65 batch loss 0.396044493 epoch total loss 0.41502586\n",
      "Trained batch 66 batch loss 0.372843951 epoch total loss 0.414386749\n",
      "Trained batch 67 batch loss 0.375568449 epoch total loss 0.413807362\n",
      "Trained batch 68 batch loss 0.389157891 epoch total loss 0.413444877\n",
      "Trained batch 69 batch loss 0.374839187 epoch total loss 0.412885398\n",
      "Trained batch 70 batch loss 0.360201865 epoch total loss 0.41213277\n",
      "Trained batch 71 batch loss 0.378424764 epoch total loss 0.411658019\n",
      "Trained batch 72 batch loss 0.367992759 epoch total loss 0.411051571\n",
      "Trained batch 73 batch loss 0.353271961 epoch total loss 0.410260051\n",
      "Trained batch 74 batch loss 0.346978843 epoch total loss 0.409404904\n",
      "Trained batch 75 batch loss 0.347451478 epoch total loss 0.408578873\n",
      "Trained batch 76 batch loss 0.383146256 epoch total loss 0.408244222\n",
      "Trained batch 77 batch loss 0.3977 epoch total loss 0.408107281\n",
      "Trained batch 78 batch loss 0.383865386 epoch total loss 0.407796502\n",
      "Trained batch 79 batch loss 0.378239602 epoch total loss 0.407422364\n",
      "Trained batch 80 batch loss 0.385804743 epoch total loss 0.407152116\n",
      "Trained batch 81 batch loss 0.336522579 epoch total loss 0.40628013\n",
      "Trained batch 82 batch loss 0.378047675 epoch total loss 0.405935854\n",
      "Trained batch 83 batch loss 0.380934477 epoch total loss 0.405634642\n",
      "Trained batch 84 batch loss 0.362589806 epoch total loss 0.405122221\n",
      "Trained batch 85 batch loss 0.382111192 epoch total loss 0.404851496\n",
      "Trained batch 86 batch loss 0.365679771 epoch total loss 0.404396027\n",
      "Trained batch 87 batch loss 0.358292311 epoch total loss 0.403866082\n",
      "Trained batch 88 batch loss 0.350730777 epoch total loss 0.403262258\n",
      "Trained batch 89 batch loss 0.371463 epoch total loss 0.402905\n",
      "Trained batch 90 batch loss 0.370497823 epoch total loss 0.402544916\n",
      "Trained batch 91 batch loss 0.358760357 epoch total loss 0.402063757\n",
      "Trained batch 92 batch loss 0.395326793 epoch total loss 0.401990563\n",
      "Trained batch 93 batch loss 0.409292281 epoch total loss 0.402069092\n",
      "Trained batch 94 batch loss 0.399658173 epoch total loss 0.402043432\n",
      "Trained batch 95 batch loss 0.393555403 epoch total loss 0.401954085\n",
      "Trained batch 96 batch loss 0.393967032 epoch total loss 0.401870877\n",
      "Trained batch 97 batch loss 0.396039188 epoch total loss 0.401810765\n",
      "Trained batch 98 batch loss 0.384912044 epoch total loss 0.401638299\n",
      "Trained batch 99 batch loss 0.384457141 epoch total loss 0.40146476\n",
      "Trained batch 100 batch loss 0.410341024 epoch total loss 0.401553512\n",
      "Trained batch 101 batch loss 0.388228714 epoch total loss 0.401421577\n",
      "Trained batch 102 batch loss 0.376457334 epoch total loss 0.40117684\n",
      "Trained batch 103 batch loss 0.37734288 epoch total loss 0.400945425\n",
      "Trained batch 104 batch loss 0.374803752 epoch total loss 0.400694072\n",
      "Trained batch 105 batch loss 0.376648843 epoch total loss 0.400465071\n",
      "Trained batch 106 batch loss 0.376945943 epoch total loss 0.400243193\n",
      "Trained batch 107 batch loss 0.375962377 epoch total loss 0.400016248\n",
      "Trained batch 108 batch loss 0.385123819 epoch total loss 0.399878353\n",
      "Trained batch 109 batch loss 0.394273371 epoch total loss 0.399826914\n",
      "Trained batch 110 batch loss 0.3807607 epoch total loss 0.399653584\n",
      "Trained batch 111 batch loss 0.393439293 epoch total loss 0.399597615\n",
      "Trained batch 112 batch loss 0.375195861 epoch total loss 0.39937973\n",
      "Trained batch 113 batch loss 0.356876433 epoch total loss 0.399003595\n",
      "Trained batch 114 batch loss 0.373740226 epoch total loss 0.398781985\n",
      "Trained batch 115 batch loss 0.356749326 epoch total loss 0.398416489\n",
      "Trained batch 116 batch loss 0.355949342 epoch total loss 0.398050398\n",
      "Trained batch 117 batch loss 0.370826751 epoch total loss 0.397817731\n",
      "Trained batch 118 batch loss 0.372094929 epoch total loss 0.397599727\n",
      "Trained batch 119 batch loss 0.371502638 epoch total loss 0.397380412\n",
      "Trained batch 120 batch loss 0.389891416 epoch total loss 0.397318\n",
      "Trained batch 121 batch loss 0.369138122 epoch total loss 0.3970851\n",
      "Trained batch 122 batch loss 0.391145468 epoch total loss 0.397036403\n",
      "Trained batch 123 batch loss 0.376106858 epoch total loss 0.396866262\n",
      "Trained batch 124 batch loss 0.38586396 epoch total loss 0.396777511\n",
      "Trained batch 125 batch loss 0.38377735 epoch total loss 0.39667353\n",
      "Trained batch 126 batch loss 0.385999262 epoch total loss 0.396588802\n",
      "Trained batch 127 batch loss 0.367205203 epoch total loss 0.396357447\n",
      "Trained batch 128 batch loss 0.360753089 epoch total loss 0.396079272\n",
      "Trained batch 129 batch loss 0.339651257 epoch total loss 0.395641863\n",
      "Trained batch 130 batch loss 0.36759311 epoch total loss 0.395426095\n",
      "Trained batch 131 batch loss 0.374853641 epoch total loss 0.395269066\n",
      "Trained batch 132 batch loss 0.367720157 epoch total loss 0.39506036\n",
      "Trained batch 133 batch loss 0.356399417 epoch total loss 0.394769669\n",
      "Trained batch 134 batch loss 0.374160349 epoch total loss 0.394615889\n",
      "Trained batch 135 batch loss 0.358362377 epoch total loss 0.39434734\n",
      "Trained batch 136 batch loss 0.367028207 epoch total loss 0.394146472\n",
      "Trained batch 137 batch loss 0.384665817 epoch total loss 0.394077271\n",
      "Trained batch 138 batch loss 0.350567728 epoch total loss 0.393762\n",
      "Trained batch 139 batch loss 0.373525053 epoch total loss 0.393616378\n",
      "Trained batch 140 batch loss 0.367260903 epoch total loss 0.393428117\n",
      "Trained batch 141 batch loss 0.358813375 epoch total loss 0.393182635\n",
      "Trained batch 142 batch loss 0.400288105 epoch total loss 0.393232673\n",
      "Trained batch 143 batch loss 0.379371166 epoch total loss 0.393135726\n",
      "Trained batch 144 batch loss 0.396238506 epoch total loss 0.393157303\n",
      "Trained batch 145 batch loss 0.371553063 epoch total loss 0.393008292\n",
      "Trained batch 146 batch loss 0.342881143 epoch total loss 0.392664939\n",
      "Trained batch 147 batch loss 0.368876517 epoch total loss 0.392503142\n",
      "Trained batch 148 batch loss 0.355278373 epoch total loss 0.392251611\n",
      "Trained batch 149 batch loss 0.348312885 epoch total loss 0.391956717\n",
      "Trained batch 150 batch loss 0.357332885 epoch total loss 0.391725898\n",
      "Trained batch 151 batch loss 0.367147774 epoch total loss 0.391563147\n",
      "Trained batch 152 batch loss 0.384320438 epoch total loss 0.391515493\n",
      "Trained batch 153 batch loss 0.396429181 epoch total loss 0.39154762\n",
      "Trained batch 154 batch loss 0.364865959 epoch total loss 0.39137435\n",
      "Trained batch 155 batch loss 0.38112849 epoch total loss 0.391308248\n",
      "Trained batch 156 batch loss 0.377096057 epoch total loss 0.391217142\n",
      "Trained batch 157 batch loss 0.361513287 epoch total loss 0.391027957\n",
      "Trained batch 158 batch loss 0.386591136 epoch total loss 0.390999883\n",
      "Trained batch 159 batch loss 0.348872364 epoch total loss 0.390734941\n",
      "Trained batch 160 batch loss 0.384096593 epoch total loss 0.390693456\n",
      "Trained batch 161 batch loss 0.340594411 epoch total loss 0.39038229\n",
      "Trained batch 162 batch loss 0.36487326 epoch total loss 0.390224814\n",
      "Trained batch 163 batch loss 0.35407576 epoch total loss 0.390003055\n",
      "Trained batch 164 batch loss 0.374992609 epoch total loss 0.389911503\n",
      "Trained batch 165 batch loss 0.34661296 epoch total loss 0.389649093\n",
      "Trained batch 166 batch loss 0.356938481 epoch total loss 0.38945204\n",
      "Trained batch 167 batch loss 0.339387923 epoch total loss 0.389152259\n",
      "Trained batch 168 batch loss 0.342081338 epoch total loss 0.388872057\n",
      "Trained batch 169 batch loss 0.354656488 epoch total loss 0.38866961\n",
      "Trained batch 170 batch loss 0.344873786 epoch total loss 0.388411969\n",
      "Trained batch 171 batch loss 0.351717353 epoch total loss 0.388197392\n",
      "Trained batch 172 batch loss 0.323218584 epoch total loss 0.387819588\n",
      "Trained batch 173 batch loss 0.321739584 epoch total loss 0.387437642\n",
      "Trained batch 174 batch loss 0.324179143 epoch total loss 0.387074083\n",
      "Trained batch 175 batch loss 0.375511259 epoch total loss 0.387008\n",
      "Trained batch 176 batch loss 0.391270429 epoch total loss 0.387032241\n",
      "Trained batch 177 batch loss 0.377002478 epoch total loss 0.386975557\n",
      "Trained batch 178 batch loss 0.393802524 epoch total loss 0.387013882\n",
      "Trained batch 179 batch loss 0.389702022 epoch total loss 0.387028903\n",
      "Trained batch 180 batch loss 0.381590724 epoch total loss 0.386998713\n",
      "Trained batch 181 batch loss 0.360043555 epoch total loss 0.386849791\n",
      "Trained batch 182 batch loss 0.346832246 epoch total loss 0.386629909\n",
      "Trained batch 183 batch loss 0.364803404 epoch total loss 0.38651067\n",
      "Trained batch 184 batch loss 0.363379091 epoch total loss 0.386384964\n",
      "Trained batch 185 batch loss 0.361141682 epoch total loss 0.386248529\n",
      "Trained batch 186 batch loss 0.377471685 epoch total loss 0.386201352\n",
      "Trained batch 187 batch loss 0.37751627 epoch total loss 0.38615492\n",
      "Trained batch 188 batch loss 0.357106715 epoch total loss 0.386000425\n",
      "Trained batch 189 batch loss 0.354757339 epoch total loss 0.385835111\n",
      "Trained batch 190 batch loss 0.368767202 epoch total loss 0.385745287\n",
      "Trained batch 191 batch loss 0.410033554 epoch total loss 0.385872453\n",
      "Trained batch 192 batch loss 0.374969214 epoch total loss 0.38581565\n",
      "Trained batch 193 batch loss 0.384084225 epoch total loss 0.38580671\n",
      "Trained batch 194 batch loss 0.388770193 epoch total loss 0.385821968\n",
      "Trained batch 195 batch loss 0.370452344 epoch total loss 0.385743171\n",
      "Trained batch 196 batch loss 0.378238082 epoch total loss 0.385704845\n",
      "Trained batch 197 batch loss 0.379963547 epoch total loss 0.385675728\n",
      "Trained batch 198 batch loss 0.389647812 epoch total loss 0.385695785\n",
      "Trained batch 199 batch loss 0.40063858 epoch total loss 0.385770857\n",
      "Trained batch 200 batch loss 0.392265081 epoch total loss 0.385803342\n",
      "Trained batch 201 batch loss 0.382572055 epoch total loss 0.385787249\n",
      "Trained batch 202 batch loss 0.393161803 epoch total loss 0.385823756\n",
      "Trained batch 203 batch loss 0.388610184 epoch total loss 0.385837495\n",
      "Trained batch 204 batch loss 0.353098869 epoch total loss 0.385677\n",
      "Trained batch 205 batch loss 0.32399255 epoch total loss 0.385376096\n",
      "Trained batch 206 batch loss 0.373310834 epoch total loss 0.385317534\n",
      "Trained batch 207 batch loss 0.385762036 epoch total loss 0.38531971\n",
      "Trained batch 208 batch loss 0.369674355 epoch total loss 0.385244489\n",
      "Trained batch 209 batch loss 0.365037203 epoch total loss 0.38514778\n",
      "Trained batch 210 batch loss 0.345259249 epoch total loss 0.38495785\n",
      "Trained batch 211 batch loss 0.344086498 epoch total loss 0.384764135\n",
      "Trained batch 212 batch loss 0.342316389 epoch total loss 0.384563923\n",
      "Trained batch 213 batch loss 0.348436326 epoch total loss 0.384394288\n",
      "Trained batch 214 batch loss 0.338976711 epoch total loss 0.384182036\n",
      "Trained batch 215 batch loss 0.333125323 epoch total loss 0.383944571\n",
      "Trained batch 216 batch loss 0.347810864 epoch total loss 0.383777261\n",
      "Trained batch 217 batch loss 0.359515 epoch total loss 0.383665442\n",
      "Trained batch 218 batch loss 0.378781408 epoch total loss 0.383643061\n",
      "Trained batch 219 batch loss 0.370945632 epoch total loss 0.383585095\n",
      "Trained batch 220 batch loss 0.374049872 epoch total loss 0.383541733\n",
      "Trained batch 221 batch loss 0.365160137 epoch total loss 0.383458555\n",
      "Trained batch 222 batch loss 0.356266677 epoch total loss 0.383336067\n",
      "Trained batch 223 batch loss 0.373667181 epoch total loss 0.383292705\n",
      "Trained batch 224 batch loss 0.376684487 epoch total loss 0.383263201\n",
      "Trained batch 225 batch loss 0.357176781 epoch total loss 0.383147269\n",
      "Trained batch 226 batch loss 0.352960646 epoch total loss 0.383013695\n",
      "Trained batch 227 batch loss 0.402911544 epoch total loss 0.383101344\n",
      "Trained batch 228 batch loss 0.390548259 epoch total loss 0.383134\n",
      "Trained batch 229 batch loss 0.348574072 epoch total loss 0.382983088\n",
      "Trained batch 230 batch loss 0.364624 epoch total loss 0.382903248\n",
      "Trained batch 231 batch loss 0.35792464 epoch total loss 0.382795125\n",
      "Trained batch 232 batch loss 0.386695802 epoch total loss 0.382811934\n",
      "Trained batch 233 batch loss 0.35904032 epoch total loss 0.38270992\n",
      "Trained batch 234 batch loss 0.37139219 epoch total loss 0.382661551\n",
      "Trained batch 235 batch loss 0.352742791 epoch total loss 0.382534236\n",
      "Trained batch 236 batch loss 0.366462708 epoch total loss 0.382466137\n",
      "Trained batch 237 batch loss 0.353123814 epoch total loss 0.382342339\n",
      "Trained batch 238 batch loss 0.367832363 epoch total loss 0.382281393\n",
      "Trained batch 239 batch loss 0.363616914 epoch total loss 0.382203311\n",
      "Trained batch 240 batch loss 0.364255965 epoch total loss 0.382128537\n",
      "Trained batch 241 batch loss 0.355125844 epoch total loss 0.38201648\n",
      "Trained batch 242 batch loss 0.360826761 epoch total loss 0.381928921\n",
      "Trained batch 243 batch loss 0.377374619 epoch total loss 0.381910145\n",
      "Trained batch 244 batch loss 0.384412706 epoch total loss 0.381920427\n",
      "Trained batch 245 batch loss 0.377384305 epoch total loss 0.38190192\n",
      "Trained batch 246 batch loss 0.358236879 epoch total loss 0.381805718\n",
      "Trained batch 247 batch loss 0.343999207 epoch total loss 0.381652683\n",
      "Trained batch 248 batch loss 0.357769728 epoch total loss 0.381556392\n",
      "Trained batch 249 batch loss 0.350821525 epoch total loss 0.38143295\n",
      "Trained batch 250 batch loss 0.364851028 epoch total loss 0.38136664\n",
      "Trained batch 251 batch loss 0.37442252 epoch total loss 0.381338954\n",
      "Trained batch 252 batch loss 0.414847255 epoch total loss 0.381471932\n",
      "Trained batch 253 batch loss 0.383077502 epoch total loss 0.38147828\n",
      "Trained batch 254 batch loss 0.383389145 epoch total loss 0.38148582\n",
      "Trained batch 255 batch loss 0.393058538 epoch total loss 0.381531209\n",
      "Trained batch 256 batch loss 0.382826179 epoch total loss 0.381536275\n",
      "Trained batch 257 batch loss 0.390912801 epoch total loss 0.381572753\n",
      "Trained batch 258 batch loss 0.40043959 epoch total loss 0.381645888\n",
      "Trained batch 259 batch loss 0.363144547 epoch total loss 0.381574452\n",
      "Trained batch 260 batch loss 0.342294931 epoch total loss 0.381423354\n",
      "Trained batch 261 batch loss 0.352413595 epoch total loss 0.381312221\n",
      "Trained batch 262 batch loss 0.338921547 epoch total loss 0.381150424\n",
      "Trained batch 263 batch loss 0.344421476 epoch total loss 0.381010771\n",
      "Trained batch 264 batch loss 0.361799628 epoch total loss 0.380938023\n",
      "Trained batch 265 batch loss 0.340390086 epoch total loss 0.380785018\n",
      "Trained batch 266 batch loss 0.352066755 epoch total loss 0.380677044\n",
      "Trained batch 267 batch loss 0.372508973 epoch total loss 0.380646437\n",
      "Trained batch 268 batch loss 0.362886608 epoch total loss 0.380580157\n",
      "Trained batch 269 batch loss 0.385283679 epoch total loss 0.380597651\n",
      "Trained batch 270 batch loss 0.361544698 epoch total loss 0.380527079\n",
      "Trained batch 271 batch loss 0.345209897 epoch total loss 0.380396754\n",
      "Trained batch 272 batch loss 0.342357218 epoch total loss 0.380256891\n",
      "Trained batch 273 batch loss 0.333137095 epoch total loss 0.380084276\n",
      "Trained batch 274 batch loss 0.337019384 epoch total loss 0.379927099\n",
      "Trained batch 275 batch loss 0.323598 epoch total loss 0.379722297\n",
      "Trained batch 276 batch loss 0.362241626 epoch total loss 0.379658967\n",
      "Trained batch 277 batch loss 0.350774556 epoch total loss 0.379554689\n",
      "Trained batch 278 batch loss 0.326198518 epoch total loss 0.379362762\n",
      "Trained batch 279 batch loss 0.33994776 epoch total loss 0.379221499\n",
      "Trained batch 280 batch loss 0.364711344 epoch total loss 0.379169643\n",
      "Trained batch 281 batch loss 0.318811059 epoch total loss 0.378954858\n",
      "Trained batch 282 batch loss 0.335303873 epoch total loss 0.378800064\n",
      "Trained batch 283 batch loss 0.366896868 epoch total loss 0.378758\n",
      "Trained batch 284 batch loss 0.36064139 epoch total loss 0.378694206\n",
      "Trained batch 285 batch loss 0.371201396 epoch total loss 0.378667921\n",
      "Trained batch 286 batch loss 0.367948294 epoch total loss 0.378630459\n",
      "Trained batch 287 batch loss 0.385157555 epoch total loss 0.378653169\n",
      "Trained batch 288 batch loss 0.372991711 epoch total loss 0.378633529\n",
      "Trained batch 289 batch loss 0.344528377 epoch total loss 0.378515512\n",
      "Trained batch 290 batch loss 0.340179771 epoch total loss 0.378383309\n",
      "Trained batch 291 batch loss 0.388068557 epoch total loss 0.378416598\n",
      "Trained batch 292 batch loss 0.38132143 epoch total loss 0.378426552\n",
      "Trained batch 293 batch loss 0.356206 epoch total loss 0.378350735\n",
      "Trained batch 294 batch loss 0.336031824 epoch total loss 0.378206789\n",
      "Trained batch 295 batch loss 0.345048606 epoch total loss 0.378094375\n",
      "Trained batch 296 batch loss 0.359583795 epoch total loss 0.37803182\n",
      "Trained batch 297 batch loss 0.329910576 epoch total loss 0.377869815\n",
      "Trained batch 298 batch loss 0.353842318 epoch total loss 0.37778917\n",
      "Trained batch 299 batch loss 0.334487528 epoch total loss 0.37764436\n",
      "Trained batch 300 batch loss 0.332704812 epoch total loss 0.377494544\n",
      "Trained batch 301 batch loss 0.338561356 epoch total loss 0.377365202\n",
      "Trained batch 302 batch loss 0.340781301 epoch total loss 0.377244085\n",
      "Trained batch 303 batch loss 0.327701747 epoch total loss 0.37708059\n",
      "Trained batch 304 batch loss 0.364996046 epoch total loss 0.377040833\n",
      "Trained batch 305 batch loss 0.348266125 epoch total loss 0.376946509\n",
      "Trained batch 306 batch loss 0.373705298 epoch total loss 0.376935899\n",
      "Trained batch 307 batch loss 0.355826914 epoch total loss 0.376867145\n",
      "Trained batch 308 batch loss 0.372804 epoch total loss 0.376853943\n",
      "Trained batch 309 batch loss 0.38952741 epoch total loss 0.376894951\n",
      "Trained batch 310 batch loss 0.35931018 epoch total loss 0.376838237\n",
      "Trained batch 311 batch loss 0.343631506 epoch total loss 0.376731455\n",
      "Trained batch 312 batch loss 0.3560175 epoch total loss 0.376665056\n",
      "Trained batch 313 batch loss 0.37211898 epoch total loss 0.376650542\n",
      "Trained batch 314 batch loss 0.393229634 epoch total loss 0.376703322\n",
      "Trained batch 315 batch loss 0.378557771 epoch total loss 0.376709193\n",
      "Trained batch 316 batch loss 0.354851782 epoch total loss 0.376640022\n",
      "Trained batch 317 batch loss 0.354477853 epoch total loss 0.376570106\n",
      "Trained batch 318 batch loss 0.364847273 epoch total loss 0.37653324\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "#PROJECT_PATH = os.getenv('HOME') + '/aiffel/CV-PoseEstimation'\n",
    "#MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1171a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abef6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55907010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5227254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b28215e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
